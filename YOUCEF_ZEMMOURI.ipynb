{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYNAMIC PROGRAMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Dynamic_Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different functions used to do this are available in **Dynamic_Programming.py**.\n",
    "- *OptimalBellmanValue* where we compute $V \\mapsto \\mathcal{T}V$\n",
    "- *OptimalBellmanPolicy* where we compute the optimal policy $\\pi(x) \\in argmax_{a \\in A}[r(x,a)+\\gamma \\sum_y p(y~|~x,a)V^{\\pi}(x)]]$\n",
    "- *PolicyEvaluation* where we compute the Value from any policy $\\pi$\n",
    "- *ValueIteratin* and *PolicyIteratin*\n",
    "- *PlotConvergenceValue* and *PlotConvergencePolicy* where we plot the convergence of $\\|v^{k+1} - v^k\\|_{\\infty}$ using two different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Implement the discrete MDP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([[-0.4,0],[2,0],[-1,-0.5]])\n",
    "P = np.array([\n",
    "            [[0.45,0,0.55],[0,0,1],[0.6,0,0.4]]\n",
    "            ,\n",
    "            [[0,0,1],[0.5,0.4,0.1],[0,0.9,0.1]]\n",
    "            ])\n",
    "Actions = [0,1]\n",
    "States = [0,1,2]\n",
    "\n",
    "Model = Dynamic_Programming.MarkovDecisionProcess(States,Actions,P,R,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement and run value iteration (VI) in order to identify a 0.01-optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
